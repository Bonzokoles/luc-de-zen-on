# ü§ñ HuggingFace MiniHelpers - Lekkie Modele i Narzƒôdzia

**Data utworzenia:** 2026-02-09  
**Projekt:** ZENON_Biznes_HUB (luc-de-zen-on)  
**Cel:** Integracja lekkich modeli i narzƒôdzi z HuggingFace dla wsparcia funkcji biznesowych

---

## üìã Spis Tre≈õci

1. [Co to sƒÖ MiniHelpers?](#co-to-sa)
2. [Rekomendowane Modele](#modele)
3. [Przypadki U≈ºycia](#przypadki-uzycia)
4. [Integracja z Projektem](#integracja)
5. [Konfiguracja](#konfiguracja)
6. [Przyk≈Çady Implementacji](#przyklady)
7. [Nastƒôpne Kroki](#nastepne-kroki)

---

## <a name="co-to-sa"></a>üéØ Co to sƒÖ MiniHelpers?

**MiniHelpers** to zbi√≥r lekkich, wydajnych modeli AI z HuggingFace, kt√≥re:
- Dzia≈ÇajƒÖ szybko nawet na s≈Çabszym sprzƒôcie
- ZajmujƒÖ ma≈Ço pamiƒôci (80MB-500MB)
- Nie wymagajƒÖ GPU do inference
- Idealnie nadajƒÖ siƒô do zada≈Ñ pomocniczych w aplikacji biznesowej

### Dlaczego MiniHelpers?

W projekcie ZENON_Biznes_HUB mamy ju≈º du≈ºe modele (Gemini, GPT-4, Claude), ale brakuje:
- ‚úÖ **Szybkich embeddings** dla wyszukiwania semantycznego
- ‚úÖ **Klasyfikacji tekstu** dla kategoryzacji tre≈õci
- ‚úÖ **Ekstrakcji informacji** z dokument√≥w
- ‚úÖ **T≈Çumacze≈Ñ** polsko-angielskich
- ‚úÖ **Analizy sentymentu** opinii klient√≥w

---

## <a name="modele"></a>üîß Rekomendowane Modele z HuggingFace

### 1. **SmolLM (Hugging Face SmolLM)**
**Repo:** https://github.com/huggingface/smollm  
**Model:** `HuggingFaceTB/SmolLM2-1.7B-Instruct`

**Parametry:**
- Rozmiar: 1.7B parametr√≥w
- RAM: ~4GB
- Licencja: Apache 2.0 (komercyjne OK)
- Jƒôzyki: EN, PL (czƒô≈õciowo)

**U≈ºycie:**
```javascript
// Endpoint API dla SmolLM
// src/pages/api/smollm-helper.ts
const MODEL_ID = "HuggingFaceTB/SmolLM2-1.7B-Instruct";
```

**Przypadki u≈ºycia:**
- Szybkie odpowiedzi na proste pytania
- Klasyfikacja tekstu (spam/nie-spam, kategoria)
- Generowanie kr√≥tkich opis√≥w
- T≈Çumaczenia prostych fraz

---

### 2. **sentence-transformers/all-MiniLM-L6-v2**
**Repo:** https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

**Parametry:**
- Rozmiar: ~80MB
- Bardzo szybki (CPU OK)
- Embeddings: 384 wymiary

**U≈ºycie:**
```python
# Dla funkcji wyszukiwania semantycznego
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
```

**Przypadki u≈ºycia:**
- Wyszukiwanie semantyczne w dokumentach
- Podobie≈Ñstwo tekst√≥w (duplikaty, FAQ matching)
- Clustering tre≈õci biznesowych
- Rekomendacje artyku≈Ç√≥w/produkt√≥w

---

### 3. **BAAI/bge-small-en-v1.5**
**Repo:** https://huggingface.co/BAAI/bge-small-en-v1.5

**Parametry:**
- Rozmiar: ~133MB
- Embeddings: 384 wymiary
- Optymalizowany dla retrieval

**U≈ºycie:**
```javascript
// Embedding API endpoint
const MODEL = "BAAI/bge-small-en-v1.5";
```

**Przypadki u≈ºycia:**
- RAG (Retrieval Augmented Generation)
- Semantyczne wyszukiwanie w bazie wiedzy
- Q&A matching

---

### 4. **Xenova/gte-small**
**Repo:** https://huggingface.co/Xenova/gte-small

**Parametry:**
- Rozmiar: ~120MB
- Obs≈Çuga ONNX (szybki inference w przeglƒÖdarce!)
- Wielojƒôzyczny

**U≈ºycie:**
```javascript
// Client-side embeddings (Transformers.js)
import { pipeline } from '@xenova/transformers';
const embedder = await pipeline('feature-extraction', 'Xenova/gte-small');
```

**Przypadki u≈ºycia:**
- Embeddings w przeglƒÖdarce (bez API!)
- Offline wyszukiwanie
- Privacy-first processing

---

### 5. **sdadas/polish-roberta-base-v2**
**Repo:** https://huggingface.co/sdadas/polish-roberta-base-v2

**Parametry:**
- Rozmiar: ~500MB
- **POLSKI MODEL** (natywnie)
- Sentiment, NER, Classification

**U≈ºycie:**
```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("sdadas/polish-roberta-base-v2")
model = AutoModel.from_pretrained("sdadas/polish-roberta-base-v2")
```

**Przypadki u≈ºycia:**
- Analiza sentymentu polskich opinii
- Named Entity Recognition (NER) - firmy, osoby, lokalizacje
- Klasyfikacja polskich tekst√≥w biznesowych

---

### 6. **Bielik-11B-v2.2-Instruct** (ju≈º planowany)
**Repo:** https://huggingface.co/speakleash/Bielik-11B-v2.2-Instruct

**Status:** ‚úÖ Dokumentacja istnieje (`GATEWAY_BIELIK_SETUP.md`)  
**Integracja:** Przez Cloudflare AI Gateway  
**Token:** `HF_TOKEN` (environment variable)

**Przeznaczenie:**
- Voice Assistant (polskie rozmowy)
- Agent Orchestrator (routing zada≈Ñ)
- G≈Ç√≥wny polski LLM w systemie

---

## <a name="przypadki-uzycia"></a>üí° Przypadki U≈ºycia w ZENON_Biznes_HUB

### 1. **Wyszukiwanie Semantyczne w Dokumentach**
**Model:** `all-MiniLM-L6-v2` lub `bge-small-en-v1.5`

**Funkcjonalno≈õƒá:**
- U≈ºytkownik wpisuje pytanie: "jak wystawiƒá fakturƒô VAT?"
- System wyszukuje w dokumentacji najlepiej pasujƒÖce sekcje
- Zwraca top 3 wyniki z linkami

**Implementacja:**
```typescript
// src/pages/api/semantic-search.ts
export const POST: APIRoute = async ({ request, locals }) => {
  const { query } = await request.json();
  
  // 1. Generate query embedding
  const queryEmbedding = await generateEmbedding(query);
  
  // 2. Search in vector DB (lub localStorage z pre-computed embeddings)
  const results = await searchSimilarDocs(queryEmbedding);
  
  return new Response(JSON.stringify(results));
};
```

---

### 2. **Auto-kategoryzacja Tre≈õci Marketingowych**
**Model:** `SmolLM2-1.7B` lub `polish-roberta-base-v2`

**Funkcjonalno≈õƒá:**
- U≈ºytkownik generuje post marketingowy
- System automatycznie sugeruje kategoriƒô: "Promocja", "Informacja", "Event", "Oferta"
- Dodaje odpowiednie hashtagi

**Implementacja:**
```typescript
// src/pages/api/classify-content.ts
const categories = ["Promocja", "Informacja", "Event", "Oferta", "Edukacja"];

export const POST: APIRoute = async ({ request }) => {
  const { text } = await request.json();
  
  const category = await classifyText(text, categories);
  const hashtags = await generateHashtags(text, category);
  
  return new Response(JSON.stringify({ category, hashtags }));
};
```

---

### 3. **Analiza Sentymentu Opinii Klient√≥w**
**Model:** `polish-roberta-base-v2`

**Funkcjonalno≈õƒá:**
- Import opinii klient√≥w (CSV, teksty)
- Automatyczna analiza: pozytywne/neutralne/negatywne
- Dashboard z statystykami

**UI:**
```
üìä Analiza Opinii Klient√≥w
--------------------------------
‚úÖ Pozytywne: 127 (63%)
üòê Neutralne: 52 (26%)
‚ùå Negatywne: 21 (11%)

üîç Top problemy:
- Op√≥≈∫nienia w dostawie (8 opinii)
- Problemy z p≈Çatno≈õciƒÖ (5 opinii)
```

---

### 4. **Smart FAQ Matching**
**Model:** `all-MiniLM-L6-v2`

**Funkcjonalno≈õƒá:**
- U≈ºytkownik zadaje pytanie
- System znajduje najbardziej podobne pytanie z FAQ
- Automatycznie wy≈õwietla odpowied≈∫ (zamiast czatu z LLM)

**Oszczƒôdno≈õci:**
- Mniej wywo≈Ça≈Ñ do drogich API (Gemini/GPT-4)
- Szybsze odpowiedzi (embedding vs generacja)
- Offline mode mo≈ºliwy

---

### 5. **Ekstrakcja Danych z Faktur/Um√≥w**
**Model:** `SmolLM2-1.7B` + prompt engineering

**Funkcjonalno≈õƒá:**
- Upload obrazu faktury/PDF
- Ekstrakcja: kwota, data, NIP, nazwa firmy
- Auto-fill formularza ksiƒôgowego

**Przyk≈Çad:**
```json
{
  "kwota_netto": 1500.00,
  "kwota_brutto": 1845.00,
  "vat": 23,
  "data_wystawienia": "2026-02-09",
  "sprzedawca_nip": "1234567890",
  "nabywca_nazwa": "Firma XYZ Sp. z o.o."
}
```

---

## <a name="integracja"></a>üîå Integracja z Projektem

### Architektura

```
User Input (Text/File)
    ‚Üì
[Frontend React Component]
    ‚Üì
[Astro API Endpoint]
    ‚Üì
[HuggingFace Inference API]
    ‚Üì
[Mini Helper Model]
    ‚Üì
[Response Processing]
    ‚Üì
[Display Results]
```

### Opcje Deployment

#### **Opcja 1: HuggingFace Inference API** (zalecane dla start)
‚úÖ ≈Åatwa integracja  
‚úÖ Bezp≈Çatna warstwa (rate limited)  
‚úÖ Bez instalacji modeli

```typescript
// src/pages/api/hf-inference.ts
const HF_API_URL = "https://api-inference.huggingface.co/models/";
const HF_TOKEN = env.HF_TOKEN;

const response = await fetch(`${HF_API_URL}sentence-transformers/all-MiniLM-L6-v2`, {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${HF_TOKEN}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({ inputs: text })
});
```

#### **Opcja 2: Cloudflare Workers AI** (future)
‚úÖ Wiƒôcej kontroli  
‚úÖ Rate limiting customizowany  
‚úÖ Caching w AI Gateway

```typescript
// Via Cloudflare AI Gateway
const gatewayUrl = `https://gateway.ai.cloudflare.com/v1/${CF_ACCOUNT_ID}/mini-helpers-gateway/huggingface/sentence-transformers/all-MiniLM-L6-v2`;
```

#### **Opcja 3: Transformers.js (client-side)** (eksperymentalne)
‚úÖ Bez API calls  
‚úÖ Privacy-first  
‚ùå Wolniejsze na s≈Çabych urzƒÖdzeniach

```javascript
import { pipeline } from '@xenova/transformers';

// Runs in browser!
const embedder = await pipeline('feature-extraction', 'Xenova/gte-small');
const output = await embedder(text);
```

---

## <a name="konfiguracja"></a>‚öôÔ∏è Konfiguracja

### 1. Dodaj HF_TOKEN do Cloudflare

```bash
# Pobierz token z: https://huggingface.co/settings/tokens
npx wrangler pages secret put HF_TOKEN --project-name=luc-de-zen-on

# Sprawd≈∫
npx wrangler pages secret list --project-name=luc-de-zen-on
```

### 2. Zaktualizuj .env.example

```bash
# HuggingFace API Token (dla MiniHelpers)
HF_TOKEN=hf_your_token_here

# Opcjonalnie: Custom endpoint URL
HF_INFERENCE_URL=https://api-inference.huggingface.co/models/
```

### 3. Dodaj nowy endpoint API

```bash
# Stw√≥rz plik
touch src/pages/api/mini-helper-embeddings.ts
```

---

## <a name="przyklady"></a>üìù Przyk≈Çady Implementacji

### Przyk≈Çad 1: Semantic Search Endpoint

**Plik:** `src/pages/api/semantic-search.ts`

```typescript
import type { APIRoute } from 'astro';

export const POST: APIRoute = async ({ request, locals }) => {
  try {
    const { query, documents } = await request.json();
    const env = locals.runtime.env;
    
    // Validate
    if (!query || !documents || !Array.isArray(documents)) {
      return new Response(JSON.stringify({ error: 'Invalid input' }), { status: 400 });
    }
    
    // Generate embeddings for query
    const queryEmbedding = await generateEmbedding(query, env.HF_TOKEN);
    
    // Generate embeddings for all documents (can be cached)
    const docEmbeddings = await Promise.all(
      documents.map(doc => generateEmbedding(doc.text, env.HF_TOKEN))
    );
    
    // Calculate cosine similarity
    const similarities = docEmbeddings.map((docEmb, idx) => ({
      index: idx,
      score: cosineSimilarity(queryEmbedding, docEmb),
      document: documents[idx]
    }));
    
    // Sort by score descending
    const results = similarities
      .sort((a, b) => b.score - a.score)
      .slice(0, 5); // Top 5
    
    return new Response(JSON.stringify({ results }), {
      headers: { 'Content-Type': 'application/json' }
    });
    
  } catch (error: any) {
    return new Response(JSON.stringify({ error: error.message }), { status: 500 });
  }
};

async function generateEmbedding(text: string, token: string): Promise<number[]> {
  const response = await fetch(
    'https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2',
    {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ inputs: text })
    }
  );
  
  if (!response.ok) {
    throw new Error(`HF API error: ${response.status}`);
  }
  
  return await response.json();
}

function cosineSimilarity(a: number[], b: number[]): number {
  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
  const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
  const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
  return dotProduct / (magnitudeA * magnitudeB);
}
```

---

### Przyk≈Çad 2: Sentiment Analysis Endpoint

**Plik:** `src/pages/api/sentiment-analysis.ts`

```typescript
import type { APIRoute } from 'astro';

export const POST: APIRoute = async ({ request, locals }) => {
  try {
    const { text } = await request.json();
    const env = locals.runtime.env;
    
    if (!text) {
      return new Response(JSON.stringify({ error: 'Missing text' }), { status: 400 });
    }
    
    // Use Polish RoBERTa for sentiment
    const response = await fetch(
      'https://api-inference.huggingface.co/models/sdadas/polish-roberta-base-v2',
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${env.HF_TOKEN}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ inputs: text })
      }
    );
    
    if (!response.ok) {
      throw new Error(`HF API error: ${response.status}`);
    }
    
    const result = await response.json();
    
    // Process result (model-specific format)
    const sentiment = analyzeSentiment(result);
    
    return new Response(JSON.stringify({ sentiment }), {
      headers: { 'Content-Type': 'application/json' }
    });
    
  } catch (error: any) {
    return new Response(JSON.stringify({ error: error.message }), { status: 500 });
  }
};

function analyzeSentiment(modelOutput: any) {
  // Process based on model output format
  // Example: [{ label: 'POSITIVE', score: 0.95 }]
  const topResult = modelOutput[0][0];
  
  return {
    label: topResult.label,
    score: topResult.score,
    emoji: topResult.label === 'POSITIVE' ? 'üòä' : topResult.label === 'NEGATIVE' ? 'üòû' : 'üòê'
  };
}
```

---

### Przyk≈Çad 3: Text Classification Component

**Plik:** `src/components/narzedzia/TextClassifier.tsx`

```tsx
import React, { useState } from 'react';

const TextClassifier = () => {
  const [text, setText] = useState('');
  const [result, setResult] = useState<any>(null);
  const [loading, setLoading] = useState(false);

  const classifyText = async () => {
    setLoading(true);
    try {
      const response = await fetch('/api/sentiment-analysis', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text })
      });
      
      const data = await response.json();
      setResult(data.sentiment);
    } catch (error) {
      console.error('Error:', error);
    }
    setLoading(false);
  };

  return (
    <div className="card p-6">
      <h2 className="text-2xl font-bold mb-4">üß† Analiza Sentymentu</h2>
      
      <textarea
        value={text}
        onChange={(e) => setText(e.target.value)}
        placeholder="Wpisz tekst do analizy..."
        className="w-full p-3 border rounded-lg mb-4"
        rows={5}
      />
      
      <button
        onClick={classifyText}
        disabled={loading || !text}
        className="btn btn-primary"
      >
        {loading ? 'Analizujƒô...' : 'Analizuj Sentiment'}
      </button>
      
      {result && (
        <div className="mt-6 p-4 bg-gray-100 rounded-lg">
          <div className="text-6xl text-center mb-2">{result.emoji}</div>
          <div className="text-center">
            <span className="font-bold">{result.label}</span>
            <br />
            Pewno≈õƒá: {(result.score * 100).toFixed(1)}%
          </div>
        </div>
      )}
    </div>
  );
};

export default TextClassifier;
```

---

## <a name="nastepne-kroki"></a>üöÄ Nastƒôpne Kroki

### Faza 1: Setup (1-2 dni)
- [ ] ZdobƒÖd≈∫ HF_TOKEN z https://huggingface.co/settings/tokens
- [ ] Dodaj secret do Cloudflare Pages
- [ ] Stw√≥rz pierwszy endpoint: `/api/mini-helper-embeddings.ts`
- [ ] Test z modelem `all-MiniLM-L6-v2`

### Faza 2: Implementacja Semantic Search (3-5 dni)
- [ ] Endpoint `/api/semantic-search.ts`
- [ ] Pre-compute embeddings dla dokumentacji projektu
- [ ] Zapisz embeddings w localStorage lub KV
- [ ] UI: Search bar z semantic results
- [ ] Test: "jak wystawiƒá fakturƒô?" ‚Üí relevant docs

### Faza 3: Sentiment Analysis (2-3 dni)
- [ ] Endpoint `/api/sentiment-analysis.ts`
- [ ] Model: `sdadas/polish-roberta-base-v2`
- [ ] UI: Textarea + analyze button
- [ ] Dashboard: batch analysis (upload CSV)

### Faza 4: Text Classification (2-3 dni)
- [ ] Endpoint `/api/classify-content.ts`
- [ ] Kategorie: Promocja, Info, Event, Oferta
- [ ] Auto-hashtag generation
- [ ] Integracja z Generator Tre≈õci

### Faza 5: Optimization (ongoing)
- [ ] Caching embeddings (Cloudflare KV)
- [ ] Rate limiting (max 100 requests/min)
- [ ] Fallback na cached results gdy quota exceeded
- [ ] Monitoring u≈ºycia HF API

---

## üìö Przydatne Linki

### Dokumentacja
- **HuggingFace Inference API:** https://huggingface.co/docs/api-inference
- **SmolLM GitHub:** https://github.com/huggingface/smollm
- **Sentence Transformers:** https://www.sbert.net
- **Transformers.js:** https://github.com/xenova/transformers.js

### Modele
- **all-MiniLM-L6-v2:** https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
- **bge-small-en-v1.5:** https://huggingface.co/BAAI/bge-small-en-v1.5
- **polish-roberta-base-v2:** https://huggingface.co/sdadas/polish-roberta-base-v2
- **SmolLM2-1.7B-Instruct:** https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct

### Tools
- **Text Embeddings Inference:** https://github.com/huggingface/text-embeddings-inference
- **Cloudflare AI Gateway:** https://developers.cloudflare.com/ai-gateway

---

## ‚ö†Ô∏è Wa≈ºne Uwagi

1. **Rate Limiting:** HuggingFace Inference API ma limit request√≥w:
   - Free tier: ~1000 requests/day
   - Rozwa≈º caching embeddings w Cloudflare KV

2. **Cold Start:** Pierwszy request do modelu mo≈ºe trwaƒá 10-20s (model loading)
   - Kolejne requesty: 1-3s
   - U≈ºyj loading indicators w UI

3. **Quota Management:**
   - Monitor u≈ºycie na https://huggingface.co/settings/billing
   - Fallback na cached results

4. **Privacy:**
   - Je≈õli przetwarzasz wra≈ºliwe dane, u≈ºyj self-hosted models
   - Opcja: Transformers.js (client-side, bez wysy≈Çania do API)

---

**Utworzono:** 2026-02-09  
**Autor:** GitHub Copilot  
**Status:** üìã Ready for implementation
